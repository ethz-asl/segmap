{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from autoencoder import model\n",
    "import pickle\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "PLOTTING_SUPPORT = True\n",
    "RUN_AS_PY_SCRIPT = False\n",
    "SET_EULER_PARAMS = False\n",
    "SET_MARMOT_PARAMS = False\n",
    "\n",
    "# Handle arguments (When executed as .py script)\n",
    "import sys\n",
    "argv = sys.argv[:]\n",
    "if len(argv) > 1:\n",
    "  script_path = argv.pop(0)\n",
    "  if \"--euler\" in argv:\n",
    "    import sys\n",
    "    sys.stdout = open('stdout.txt', 'w')\n",
    "    RUN_AS_PY_SCRIPT = True\n",
    "    PLOTTING_SUPPORT = False\n",
    "    SET_EULER_PARAMS = True\n",
    "    print(\"Parameters set for execution on euler cluster\")\n",
    "    argv.remove(\"--euler\")\n",
    "  if \"--marmot\" in argv:\n",
    "    RUN_AS_PY_SCRIPT = True\n",
    "    PLOTTING_SUPPORT = False\n",
    "    SET_MARMOT_PARAMS = True\n",
    "    print(\"Parameters set for execution on marmot cluster\")\n",
    "    argv.remove(\"--marmot\") \n",
    "  if \"--script\" in argv:\n",
    "    RUN_AS_PY_SCRIPT = True\n",
    "    PLOTTING_SUPPORT = False\n",
    "    print(\"Running as script\")\n",
    "    argv.remove(\"--script\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  %load_ext autoreload\n",
    "  %autoreload 2\n",
    "  from IPython.display import clear_output\n",
    "  if PLOTTING_SUPPORT:\n",
    "    %matplotlib notebook\n",
    "    from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "VOXEL_SIDE = 24\n",
    "\n",
    "MAX_STEPS = 10000\n",
    "VAL_EXAMPLES = 200\n",
    "N_ROTATION_ANGLES = 12\n",
    "ROTATION_OFFSET = 0\n",
    "VAL_EVERY_N_STEPS = 1\n",
    "VAL_STEP_TOLERANCE = 3\n",
    "TRAIN_TWINS = False\n",
    "\n",
    "MP = model.ModelParams()\n",
    "MP.INPUT_SHAPE = [VOXEL_SIDE, VOXEL_SIDE, VOXEL_SIDE, 1]\n",
    "\n",
    "HOME_DIR = os.path.expanduser('~')\n",
    "DATA_DIR = \"./database/\"\n",
    "RUN_NAME = \"kitti18\"\n",
    "RESTORE_MODEL = True\n",
    "SAVE_DIR = HOME_DIR + \"/Desktop/autoencoder/\"\n",
    "SAVE_FILE = \"model.checkpoint\"\n",
    "MP_FILENAME = \"model_params.pckl\"\n",
    "TENSORBOARD_DIR = HOME_DIR + \"/tensorboard\"\n",
    "SAVE_UNVALIDATED = False\n",
    "CREATE_VISUALS = False\n",
    "DETAILED_STEP_TIMES = False\n",
    "\n",
    "EXPORT_FEATURES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if SET_EULER_PARAMS:\n",
    "    DATA_DIR = \"/cluster/home/dugasd/database/\"\n",
    "    SAVE_DIR = \"/cluster/home/dugasd/autoencoder-euler/\"\n",
    "    TENSORBOARD_DIR = None\n",
    "    CREATE_VISUALS = False\n",
    "    \n",
    "    MAX_STEPS = 1000000\n",
    "    VAL_STEP_TOLERANCE = 5\n",
    "    \n",
    "if SET_MARMOT_PARAMS:\n",
    "    DATA_DIR = \"/home/daniel/database/\"\n",
    "    RUN_NAME = \"kitti18-20-27\"\n",
    "    SAVE_DIR = \"/home/daniel/autoencoder-marmot/\"\n",
    "    TENSORBOARD_DIR = None\n",
    "    CREATE_VISUALS = False\n",
    "    \n",
    "    MAX_STEPS = 1000000\n",
    "    VAL_STEP_TOLERANCE = 10\n",
    "    N_ROTATION_ANGLES = 36\n",
    "    \n",
    "if not RUN_AS_PY_SCRIPT:\n",
    "    #MP.CONVOLUTION_LAYERS = [{'type': 'conv3d', 'filter': [5, 5, 5,  1, 10], 'downsampling': {'type': 'max_pool3d', 'k': 2}}]\n",
    "    MP.CONVOLUTION_LAYERS = []\n",
    "    #MP.LATENT_SHAPE = [2]\n",
    "    N_ROTATION_ANGLES = 6\n",
    "    CREATE_VISUALS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if RUN_AS_PY_SCRIPT:\n",
    "  while argv:\n",
    "      arg = argv.pop(0)\n",
    "      if arg == \"-RUN_NAME\":\n",
    "        RUN_NAME = argv.pop(0)\n",
    "        print(\"RUN_NAME set to \" + RUN_NAME)\n",
    "      elif arg == \"-SAVE_DIR\":\n",
    "        SAVE_DIR = argv.pop(0)\n",
    "        print(\"SAVE_DIR set to \" + SAVE_DIR)\n",
    "      elif arg == \"--noconv\":\n",
    "        MP.CONVOLUTION_LAYERS = []\n",
    "        print(\"CONVOLUTION LAYERS REMOVED\")\n",
    "      elif arg == \"--twins\":\n",
    "        TRAIN_TWINS = True\n",
    "        print(\"Training twins.\")\n",
    "      elif arg == \"-LATENT_SHAPE\":\n",
    "        MP.LATENT_SHAPE = [int(argv.pop(0))]\n",
    "        print(\"LATENT_SHAPE set to \" + str(MP.LATENT_SHAPE))\n",
    "      elif arg == \"-CLIP_GRADIENTS\":\n",
    "        MP.CLIP_GRADIENTS = float(argv.pop(0))\n",
    "        print(\"CLIP_GRADIENTS set to \" + str(MP.CLIP_GRADIENTS))\n",
    "      elif arg == \"-N_ROTATION_ANGLES\":\n",
    "        N_ROTATION_ANGLES = int(argv.pop(0))\n",
    "        print(\"N_ROTATION_ANGLES set to \" + str(N_ROTATION_ANGLES))\n",
    "      elif arg == \"-ROTATION_OFFSET\":\n",
    "        frac = list(map(float, argv.pop(0).split('/'))) + [1.0]\n",
    "        ROTATION_OFFSET = frac[0]/frac[1]\n",
    "        print(\"ROTATION_OFFSET set to \" + str(ROTATION_OFFSET))\n",
    "      elif arg == \"--float64\":\n",
    "        MP.FLOAT_TYPE = tf.float64\n",
    "        print(\"MP.FLOAT_TYPE set to \" + str(MP.FLOAT_TYPE))\n",
    "      else:\n",
    "        print(\"Unknown argument: \" + arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = SAVE_DIR+SAVE_FILE\n",
    "if SAVE_UNVALIDATED:\n",
    "  SAVE_DIR_NOVAL = SAVE_DIR+\"unvalidated/\"\n",
    "  SAVE_PATH_NOVAL = SAVE_DIR_NOVAL+SAVE_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Segments and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kitti18']\n"
     ]
    }
   ],
   "source": [
    "import utilities\n",
    "run_names, runs = utilities.list_runs(DATA_DIR)\n",
    "try:\n",
    "  run_names.remove(RUN_NAME)\n",
    "  run_names = [RUN_NAME] + run_names\n",
    "except:\n",
    "  print(RUN_NAME + \" not found in runs.\")\n",
    "print(run_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading segments, features, matches, classes for run\n",
      "  Found 1187 segments\n",
      "  Found features for 1187 segments \n",
      "  Found 56 groups of matches\n",
      "  Found 1187 segment ids\n",
      "  Found classes for 1187 segments\n"
     ]
    }
   ],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  from ipywidgets import widgets\n",
    "  run_dropdown = widgets.Dropdown(description=\"Run to import : \", options=run_names)\n",
    "  button = widgets.Button(description=\"import\")\n",
    "\n",
    "  # Interaction functions\n",
    "  def import_run_data(btn):\n",
    "    display.clear_output()\n",
    "    print(\"Loading segments, features, matches, classes for run\")\n",
    "    global segments, features, fnames, matches, classes, ids, classes_set # 'output' variables\n",
    "    segments, features, fnames, matches, classes, ids = utilities.import_run(run_dropdown.value, folder=DATA_DIR)\n",
    "    classes_set = sorted(list(set(classes)))\n",
    "  \n",
    "  button.on_click(import_run_data)\n",
    "  # Display widgets\n",
    "  from IPython import display\n",
    "  display.display(run_dropdown)\n",
    "  display.display(button)\n",
    "\n",
    "  import_run_data(button)\n",
    "else:\n",
    "  segments, features, fnames, matches, classes, ids = utilities.import_run(RUN_NAME, folder=DATA_DIR)\n",
    "  classes_set = sorted(list(set(classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  try:\n",
    "    stored_MP = pickle.load(open(SAVE_DIR+MP_FILENAME, 'rb'))\n",
    "    if MP != stored_MP: \n",
    "        print(\"WARNING: Setting params for compatibility with stored model.\")\n",
    "        print(\"Stored model: \"); print(stored_MP); print(\"New model: \"); print(MP)\n",
    "    MP = stored_MP\n",
    "  except FileNotFoundError:\n",
    "    print(\"No stored model found. Creating a new model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/daniel/Documents/catkin_ws/src/segmatch/segmatch/python/autoencoder/model.py:270 in __init__.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "vae = model.Autoencoder(MP)\n",
    "if TRAIN_TWINS: vae.build_twin_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if TENSORBOARD_DIR != None:\n",
    "  summary_writer = tf.train.SummaryWriter(TENSORBOARD_DIR, vae.sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "if RESTORE_MODEL:\n",
    "  try:\n",
    "    vae.saver.restore(vae.sess, SAVE_PATH)\n",
    "    print(\"Model restored.\")\n",
    "    print(MP.CONVOLUTION_LAYERS)\n",
    "  except:\n",
    "    print(\"Could not load model: \", end=\"\")\n",
    "    try:\n",
    "      stored_MP = pickle.load(open(SAVE_DIR+MP_FILENAME, 'rb'))\n",
    "      print(\"ERROR: mismatch between model params.\")\n",
    "      print(\"Stored model: \"); print(stored_MP); print(\"New model: \"); print(MP)\n",
    "    except:\n",
    "      print(\"no model folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Voxelized Segment Dataset - With Rotated Copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split into training and val data\n",
    "split_at = min(VAL_EXAMPLES, int(0.2 * len(ids)))\n",
    "val = segments[:split_at]\n",
    "train = segments[split_at:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxelizing training data\n"
     ]
    }
   ],
   "source": [
    "if not TRAIN_TWINS:\n",
    "  print(\"Rotating segments\")\n",
    "  from voxelize import create_rotations\n",
    "  train = create_rotations(train, N_ROTATION_ANGLES, ROTATION_OFFSET)\n",
    "  val = create_rotations(val, 12, ROTATION_OFFSET)\n",
    "\n",
    "  print(\"Voxelizing training data\")\n",
    "  from voxelize import voxelize\n",
    "  train_vox, _ = voxelize(train,VOXEL_SIDE)\n",
    "  val_vox, _   = voxelize(val  ,VOXEL_SIDE)\n",
    "\n",
    "  if train_vox[0].shape != MP.INPUT_SHAPE:\n",
    "    print(\"Reshaping\")\n",
    "    train_vox=[np.reshape(vox, MP.INPUT_SHAPE) for vox in train_vox]\n",
    "    val_vox=[np.reshape(vox, MP.INPUT_SHAPE) for vox in val_vox]\n",
    "\n",
    "  del train # Save some memory\n",
    "else:\n",
    "  from voxelize import create_twins\n",
    "  val, val_twins = create_twins(val)\n",
    "  train, train_twins = create_twins(train)\n",
    "\n",
    "  print(\"Voxelizing training data\")\n",
    "  from voxelize import voxelize\n",
    "  train_vox, _ = voxelize(train,VOXEL_SIDE)\n",
    "  val_vox, _   = voxelize(val ,VOXEL_SIDE)\n",
    "  train_twins_vox, _ = voxelize(train_twins,VOXEL_SIDE)\n",
    "  val_twins_vox, _   = voxelize(val_twins  ,VOXEL_SIDE)\n",
    " \n",
    "  del train_twins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 894.2890625mB of memory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Using \" + str(process.memory_info().rss/(1024.0*1024.0)) + \"mB of memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder ( Computationally Intensive )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous cost log found.\n",
      "Voxelizing training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/catkin_ws/src/segmatch/segmatch/python/autoencoder/model.py:390: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if twin_input != None:\n",
      "/home/daniel/Documents/catkin_ws/src/segmatch/segmatch/python/autoencoder/model.py:394: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  cost = self.cost if twin_input == None else self.twin_cost\n",
      "/home/daniel/Documents/catkin_ws/src/segmatch/segmatch/python/autoencoder/model.py:395: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  opt = self.optimizer if twin_input == None else self.twin_optimizer\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        this.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width);\n",
       "        canvas.attr('height', height);\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'];\n",
       "    var y0 = fig.canvas.height - msg['y0'];\n",
       "    var x1 = msg['x1'];\n",
       "    var y1 = fig.canvas.height - msg['y1'];\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x;\n",
       "    var y = canvas_pos.y;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation cost: 193550.413086  (Training cost: None) \n",
      "Saving ... Model saved in file: /home/daniel/Desktop/autoencoder/model.checkpoint\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Twin_Optimizer_1/beta2_power\n\t [[Node: Twin_Optimizer_1/beta2_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@EncoderLayer0Weights/weights_encoder_0\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Twin_Optimizer_1/beta2_power)]]\n\nCaused by op 'Twin_Optimizer_1/beta2_power/read', defined at:\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2831, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-910f5040faff>\", line 1, in <module>\n    vae.build_twin_graph()\n  File \"/home/daniel/Documents/catkin_ws/src/segmatch/segmatch/python/autoencoder/model.py\", line 370, in build_twin_graph\n    self.twin_optimizer = tf.train.AdamOptimizer(learning_rate=self.MP.LEARNING_RATE).minimize(self.twin_cost)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py\", line 279, in minimize\n    name=name)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py\", line 393, in apply_gradients\n    self._create_slots(var_list)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/training/adam.py\", line 116, in _create_slots\n    trainable=False)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n    expected_shape=expected_shape)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/ops/variables.py\", line 370, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1424, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Twin_Optimizer_1/beta2_power\n\t [[Node: Twin_Optimizer_1/beta2_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@EncoderLayer0Weights/weights_encoder_0\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Twin_Optimizer_1/beta2_power)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/daniel/anaconda2/envs/ml3/lib/python3.4/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    470\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Twin_Optimizer_1/beta2_power\n\t [[Node: Twin_Optimizer_1/beta2_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@EncoderLayer0Weights/weights_encoder_0\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Twin_Optimizer_1/beta2_power)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d75ebfc44d26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m       \u001b[0mt_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m       \u001b[1;31m# Train over 1 batch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m       \u001b[0mcost_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_single_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_input_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_twin_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m       \u001b[0mtotal_step_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mt_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/daniel/Documents/catkin_ws/src/segmatch/segmatch/python/autoencoder/model.py\u001b[0m in \u001b[0;36mtrain_on_single_batch\u001b[1;34m(self, batch_input, twin_input, cost_only, dropout)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m       cost, _, _ = self.sess.run((cost, opt, self.catch_nans),\n\u001b[1;32m--> 402\u001b[1;33m                                  feed_dict=dict_)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcost_on_single_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwin_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 766\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 964\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1014\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Twin_Optimizer_1/beta2_power\n\t [[Node: Twin_Optimizer_1/beta2_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@EncoderLayer0Weights/weights_encoder_0\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Twin_Optimizer_1/beta2_power)]]\n\nCaused by op 'Twin_Optimizer_1/beta2_power/read', defined at:\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2831, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-910f5040faff>\", line 1, in <module>\n    vae.build_twin_graph()\n  File \"/home/daniel/Documents/catkin_ws/src/segmatch/segmatch/python/autoencoder/model.py\", line 370, in build_twin_graph\n    self.twin_optimizer = tf.train.AdamOptimizer(learning_rate=self.MP.LEARNING_RATE).minimize(self.twin_cost)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py\", line 279, in minimize\n    name=name)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py\", line 393, in apply_gradients\n    self._create_slots(var_list)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/training/adam.py\", line 116, in _create_slots\n    trainable=False)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n    expected_shape=expected_shape)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/ops/variables.py\", line 370, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1424, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/daniel/anaconda2/envs/ml3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Twin_Optimizer_1/beta2_power\n\t [[Node: Twin_Optimizer_1/beta2_power/read = Identity[T=DT_FLOAT, _class=[\"loc:@EncoderLayer0Weights/weights_encoder_0\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Twin_Optimizer_1/beta2_power)]]\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "from autoencoder.batchmaker import Batchmaker, progress_bar\n",
    "\n",
    "total_step_cost = None\n",
    "step_cost_log = []\n",
    "total_val_cost = 0\n",
    "val_steps_since_last_improvement = 0\n",
    "step_start = timer()\n",
    "\n",
    "try:\n",
    "    val_cost_log = list(np.loadtxt(SAVE_DIR+\"val_cost_log.txt\"))\n",
    "    print(\"Previous cost log found.\")\n",
    "except:\n",
    "    val_cost_log = []\n",
    "    \n",
    "# single step\n",
    "for step in range(MAX_STEPS):\n",
    "  if TRAIN_TWINS:\n",
    "      val, val_twins = create_twins(val)\n",
    "      train, train_twins = create_twins(train)\n",
    "      print(\"Voxelizing training data\")\n",
    "      from voxelize import voxelize\n",
    "      train_vox, _ = voxelize(train,VOXEL_SIDE)\n",
    "      val_vox, _   = voxelize(val ,VOXEL_SIDE)\n",
    "      train_twins_vox, _ = voxelize(train_twins,VOXEL_SIDE)\n",
    "      val_twins_vox, _   = voxelize(val_twins  ,VOXEL_SIDE)\n",
    "      del train_twins\n",
    "  # Validation\n",
    "  val_batchmaker = Batchmaker(val_vox, val_twins_vox, BATCH_SIZE, MP)\n",
    "  if np.mod(step, VAL_EVERY_N_STEPS) == 0:\n",
    "    total_val_cost = 0\n",
    "    while True:\n",
    "      if val_batchmaker.is_depleted():\n",
    "        break\n",
    "      else:\n",
    "        batch_input_values, batch_twin_values = val_batchmaker.next_batch()\n",
    "        cost_value = vae.cost_on_single_batch(batch_input_values, batch_twin_values)\n",
    "        total_val_cost += cost_value\n",
    "        if PLOTTING_SUPPORT:\n",
    "          progress_bar(val_batchmaker)\n",
    "    print(\"Validation cost: \"+str(total_val_cost)+\"  (Training cost: \"+str(total_step_cost)+\")\", end=\"\")\n",
    "    try:\n",
    "      print(\" Step Time: \" + str(step_end-step_start))\n",
    "      if DETAILED_STEP_TIMES:\n",
    "        print(step_times)\n",
    "    except: \n",
    "        print(\" \")\n",
    "    \n",
    "    val_cost_log.append(total_val_cost)\n",
    "    \n",
    "    # Training Monitor\n",
    "    if len(val_cost_log) > 1:\n",
    "        # Save cost log.\n",
    "        import os\n",
    "        if not os.path.exists(SAVE_DIR):\n",
    "            os.makedirs(SAVE_DIR)\n",
    "            if SAVE_UNVALIDATED: os.makedirs(SAVE_DIR_NOVAL)\n",
    "            print(\"Created directory: %s\" % SAVE_DIR)\n",
    "            with open(SAVE_DIR+MP_FILENAME, 'wb') as file:\n",
    "              pickle.dump(MP, file, protocol=2)\n",
    "        np.savetxt(SAVE_DIR+\"val_cost_log.txt\", val_cost_log)\n",
    "        # Save if cost has improved. Otherwise increment counter.\n",
    "        if val_cost_log[-1] <  min(val_cost_log[:-1]):\n",
    "            val_steps_since_last_improvement = 0\n",
    "            # save model to disk\n",
    "            print(\"Saving ... \", end='')\n",
    "            save_path = vae.saver.save(vae.sess, SAVE_PATH)\n",
    "            print(\"Model saved in file: %s\" % save_path)      \n",
    "        else:\n",
    "            val_steps_since_last_improvement += 1  \n",
    "    # Stop training if val_cost hasn't improved in VAL_STEP_TOLERANCE steps\n",
    "    if val_steps_since_last_improvement > VAL_STEP_TOLERANCE:\n",
    "        if SAVE_UNVALIDATED:\n",
    "            print(\"Saving ... \", end='')\n",
    "            save_path = vae.saver.save(vae.sess, SAVE_PATH_NOVAL)\n",
    "            print(\"Unvalidated model saved in file: %s\" % save_path)\n",
    "        print(\"Training stopped by validation monitor.\")\n",
    "        break\n",
    "            \n",
    "  # Train on batches\n",
    "  step_start = timer()\n",
    "  zero = timer() - timer()\n",
    "  step_times = {'batchmaking': zero, 'training': zero, 'plotting': zero}\n",
    "  total_step_cost = 0\n",
    "  training_batchmaker = Batchmaker(train_vox, train_twins_vox, BATCH_SIZE, MP)\n",
    "  while True:\n",
    "    if training_batchmaker.is_depleted():\n",
    "      break\n",
    "    else:\n",
    "      t_a = timer()  \n",
    "      batch_input_values, batch_twin_values = training_batchmaker.next_batch()\n",
    "      t_b = timer()\n",
    "      # Train over 1 batch.\n",
    "      cost_value = vae.train_on_single_batch(batch_input_values, batch_twin_values)\n",
    "      total_step_cost += cost_value\n",
    "      t_c = timer()\n",
    "      if PLOTTING_SUPPORT:\n",
    "        progress_bar(training_batchmaker)\n",
    "      t_d = timer()\n",
    "      step_times['batchmaking'] += t_b - t_a\n",
    "      step_times['training']    += t_c - t_b\n",
    "      step_times['plotting']    += t_d - t_c\n",
    "  step_cost_log.append(total_step_cost)\n",
    "  step_end = timer()\n",
    "\n",
    "\n",
    "print(\"Training ended.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Autoencoder Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  # Plot a few random samples\n",
    "  import matplotlib.pyplot as plt\n",
    "  %matplotlib notebook\n",
    "  plt.ion()\n",
    "  n_samples = 5\n",
    "  import random\n",
    "  x_samples = random.sample(val_vox, 5)\n",
    "  x_samples = [np.reshape(sample, MP.INPUT_SHAPE) for sample in x_samples]\n",
    "  x_reconstruct = vae.encode_decode(x_samples)\n",
    "  plt.figure(figsize=(8, 12))\n",
    "  for i in range(n_samples):\n",
    "    plt.subplot(n_samples*2, 1, 2*i + 1)\n",
    "    plt.imshow(x_samples[i].reshape(VOXEL_SIDE, VOXEL_SIDE*VOXEL_SIDE), vmin=0, vmax=1, cmap='spectral')\n",
    "    plt.title(\"Top: val input - Bottom: Reconstruction\")\n",
    "    plt.subplot(n_samples*2, 1, 2*i + 2)\n",
    "    plt.imshow(x_reconstruct[i].reshape(VOXEL_SIDE, VOXEL_SIDE*VOXEL_SIDE), vmin=0, vmax=1, cmap='spectral')\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  nx = ny = 4\n",
    "  nz = 1\n",
    "  dim1 = 0\n",
    "  dim2 = 1\n",
    "  dim3 = 0\n",
    "  x_values = np.linspace(-3, 3, nx)\n",
    "  y_values = np.linspace(-3, 3, ny)\n",
    "  z_values = np.linspace(-3, 3, nz)\n",
    "  canvas = np.empty((VOXEL_SIDE*ny, VOXEL_SIDE*nx, VOXEL_SIDE*nz))\n",
    "  for i, yi in enumerate(x_values):\n",
    "      for j, xi in enumerate(y_values):\n",
    "          for k, zi in enumerate(z_values):\n",
    "              # we can only visualize 3 dimensions, in this case the first 3\n",
    "              latent_sample = np.zeros([1]+MP.LATENT_SHAPE)\n",
    "              latent_sample.flat[dim1] = xi\n",
    "              latent_sample.flat[dim2] = yi\n",
    "              latent_sample.flat[dim3] = zi\n",
    "              x_mean = vae.decode(latent_sample)\n",
    "              canvas[(nx-i-1)*VOXEL_SIDE:(nx-i)*VOXEL_SIDE,\n",
    "                     j*VOXEL_SIDE:(j+1)*VOXEL_SIDE,\n",
    "                     k*VOXEL_SIDE:(k+1)*VOXEL_SIDE] \\\n",
    "                   = x_mean[0].reshape(VOXEL_SIDE, VOXEL_SIDE, VOXEL_SIDE)\n",
    "  from mpl_toolkits.mplot3d import Axes3D\n",
    "  threshold = 0.7\n",
    "  X,Y,Z = np.where(canvas > (threshold*np.max(canvas)))\n",
    "  fig = plt.figure()\n",
    "  plt.cla()\n",
    "  ax = Axes3D(fig)\n",
    "  ax.scatter(X, Y, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Autoencoder Features for Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  print(\"Voxelizing segments\")\n",
    "  from voxelize import voxelize\n",
    "  segments_vox, features_voxel_scale = voxelize(segments, VOXEL_SIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "    print(\"Computing Eigenvalue Features\")\n",
    "    from eigenvalues import eigenvalue_features\n",
    "    features_eig = eigenvalue_features(segments)\n",
    "    features_eig[np.where(np.isnan(features_eig))] = 0\n",
    "    F = features_eig\n",
    "    C = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  print(\"Computing Features for Segments\")\n",
    "  features_nn, confusion_nn = vae.batch_encode([np.reshape(sample, MP.INPUT_SHAPE) for sample in segments_vox])\n",
    "  fnames_nn = ['autoencoder_feature'+str(i+1) for i, _ in enumerate(features_nn[0])]\n",
    "  F = features_nn\n",
    "  C = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  print(\"Rotating segments\")\n",
    "  from voxelize import create_rotations\n",
    "  rotated_segments, rotated_classes = create_rotations(segments, N_ROTATION_ANGLES, classes=classes)\n",
    "  if False: # walls_vs_cars\n",
    "    print(\"Removing unknowns\")\n",
    "    rotated_segments = [segment for segment, class_ in zip(rotated_segments, rotated_classes) if class_ != \"unknown\"]\n",
    "    rotated_classes = [class_ for class_ in rotated_classes if class_ != \"unknown\"]\n",
    "  print(\"Voxelizing rotations\")\n",
    "  from voxelize import voxelize\n",
    "  rotated_segments_vox, rotated_segments_scale = voxelize(rotated_segments, VOXEL_SIDE)\n",
    "  print(\"Computing Features for rotations\")\n",
    "  rotated_features, _ = vae.batch_encode([np.reshape(sample, MP.INPUT_SHAPE) for sample in rotated_segments_vox])\n",
    "  F = rotated_features\n",
    "  C = rotated_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not RUN_AS_PY_SCRIPT:\n",
    "  print(\"T-SNE\")\n",
    "  dir_ = \"/tmp/online_matcher/visuals/\"\n",
    "  import os\n",
    "  if not os.path.exists(dir_):\n",
    "    os.makedirs(dir_)\n",
    "  if MP.LATENT_SHAPE[0] == 2:\n",
    "    F2 = F\n",
    "  else:\n",
    "    from tools.tsne import tsne\n",
    "    F2 = tsne(F, err_threshold=1.0)\n",
    "  from itertools import cycle\n",
    "  cnames = ['dodgerblue', 'gold', 'silver', 'tomato', \n",
    "            'plum', 'lemonchiffon', 'grey', 'orchid', 'lime', 'palegreen']\n",
    "  from matplotlib import pyplot as plt\n",
    "  plt.figure(figsize=(12,7))\n",
    "  for c_, name in zip(cycle(cnames), classes_set):\n",
    "    x = [values[0] for values, class_ in zip(F2, C) if class_ == name]\n",
    "    y = [values[1] for values, class_ in zip(F2, C) if class_ == name]\n",
    "    plt.scatter(x, y, c=c_, alpha=0.8,  lw = 0)\n",
    "  box = plt.gca().get_position()\n",
    "  plt.gca().set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "  ncol = 2 if len(classes_set) > 10 else 1\n",
    "  plt.legend(classes_set, loc='center left', bbox_to_anchor=(1, 0.5), ncol=ncol)\n",
    "  plt.title('T-SNE')\n",
    "  plt.xlabel('x_dim')\n",
    "  plt.ylabel('y_dim')\n",
    "  plt.show()\n",
    "  try:\n",
    "    plt.gcf().savefig(dir_+\"t-sne.png\")\n",
    "  except:\n",
    "    print(\"not saved.\")\n",
    "  if len(matches) > 0:\n",
    "    print(\"Adding matches\")\n",
    "    # Dim all points\n",
    "    plt.cla()\n",
    "    for c_, name in zip(cycle(cnames), classes_set):\n",
    "      x = [values[0] for values, class_ in zip(F2, C) if class_ == name]\n",
    "      y = [values[1] for values, class_ in zip(F2, C) if class_ == name]\n",
    "      plt.scatter(x, y, c=c_, alpha=0.2,  lw = 0)\n",
    "    plt.legend(classes_set, loc='center left', bbox_to_anchor=(1, 0.5), ncol=ncol)\n",
    "    plt.title('T-SNE')\n",
    "    plt.xlabel('x_dim')\n",
    "    plt.ylabel('y_dim')\n",
    "    # Bring out matched points\n",
    "    matched_ids = [id_ for match in matches for id_ in match]\n",
    "    for c_, name in zip(cycle(cnames), classes_set):\n",
    "      x = [values[0] for values, class_, id_ in zip(F2, C, ids) if class_ == name and id_ in matched_ids]\n",
    "      y = [values[1] for values, class_, id_ in zip(F2, C, ids) if class_ == name and id_ in matched_ids]\n",
    "      plt.scatter(x, y, c=c_, s=30, lw = 1)\n",
    "    # Show matches as lines\n",
    "    for match in matches:\n",
    "        line_x = [ F2[ids.index(match[0])][0], F2[ids.index(match[1])][0] ]\n",
    "        line_y = [ F2[ids.index(match[0])][1], F2[ids.index(match[1])][1] ]\n",
    "        plt.plot(line_x, line_y, 'black', linewidth=1)\n",
    "        try:\n",
    "            plt.gcf().savefig(dir_+\"t-sne_matches.png\")\n",
    "        except:\n",
    "            print(\"not saved.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RC_CONFIDENCE = 0.2\n",
    "ONEVIEW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reconstructions\n",
    "if not RUN_AS_PY_SCRIPT:\n",
    "  N = 400\n",
    "  SV_ = segments_vox[:N]\n",
    "  S_ = segments[:N]\n",
    "  I_ = ids[:N]\n",
    "  reconstruction_vox = vae.batch_encode_decode([np.reshape(sample, MP.INPUT_SHAPE) for sample in SV_])\n",
    "  reconstruction_vox = [np.reshape(vox, [VOXEL_SIDE, VOXEL_SIDE, VOXEL_SIDE]) for vox in reconstruction_vox]\n",
    "  from voxelize import unvoxelize\n",
    "  reconstruction = [unvoxelize(vox > RC_CONFIDENCE) for vox in reconstruction_vox]\n",
    "  reconstruction = [segment*scale for (segment, scale) in zip(reconstruction, features_voxel_scale)]             \n",
    "  if CREATE_VISUALS:\n",
    "    dir_ = \"/tmp/online_matcher/visuals/reconstructions/\"\n",
    "    from visuals import visuals_of_matches\n",
    "    reconstruction_ids = [id_+max(I_)+1 for id_ in I_]\n",
    "    one_to_one_matches = [[id1, id2] for id1, id2 in zip(I_, reconstruction_ids)]\n",
    "    visuals_of_matches(one_to_one_matches, S_+reconstruction, I_+reconstruction_ids, directory=dir_, oneview=ONEVIEW)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reconstructions of rotations for one object\n",
    "if CREATE_VISUALS:\n",
    "  dir_ = \"/tmp/online_matcher/visuals/rotations/\"\n",
    "  class_name = \"car\"\n",
    "  class_ids = [np.random.choice([id_ for id_, class_ in zip(ids, classes) if class_ == class_name])]\n",
    "  class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "  class_segments = np.array(segments)[class_indices]\n",
    "  \n",
    "  from voxelize import create_rotations\n",
    "  class_rotated_segments = np.array(list(class_segments) + list(create_rotations(class_segments, N_ROTATION_ANGLES)))\n",
    "  from voxelize import voxelize\n",
    "  class_segments_vox, class_voxel_scale = voxelize(class_rotated_segments, VOXEL_SIDE)\n",
    "  if CREATE_VISUALS:\n",
    "    class_reconstruction_vox = vae.batch_encode_decode([np.reshape(vox, MP.INPUT_SHAPE) for vox in class_segments_vox])\n",
    "    class_reconstruction_vox = [np.reshape(vox, [VOXEL_SIDE, VOXEL_SIDE, VOXEL_SIDE]) for vox in class_reconstruction_vox]\n",
    "    from voxelize import unvoxelize\n",
    "    class_reconstruction = [unvoxelize(vox > RC_CONFIDENCE) for vox in class_reconstruction_vox]\n",
    "    class_reconstruction = [segment*scale for (segment, scale) in zip(class_reconstruction, class_voxel_scale)] \n",
    "    from visuals import visuals_of_matches\n",
    "    fake_ids = list(range(len(class_reconstruction)))\n",
    "    fake_reconstruction_ids = [id_+max(fake_ids)+1 for id_ in fake_ids]\n",
    "    one_to_one_matches = [[id1, id2] for id1, id2 in zip(fake_ids, fake_reconstruction_ids)]\n",
    "    visuals_of_matches(one_to_one_matches,\n",
    "                       list(class_rotated_segments)+class_reconstruction,\n",
    "                       fake_ids+fake_reconstruction_ids,\n",
    "                       directory=dir_, oneview=ONEVIEW)\n",
    "    clear_output()\n",
    "  class_features, confusion = vae.batch_encode([np.reshape(vox, MP.INPUT_SHAPE) for vox in class_segments_vox])\n",
    "  class_features = np.array(class_features)\n",
    "  print(class_name)\n",
    "  print(\"Id: \"+str(class_ids[0]))\n",
    "  from matplotlib import pyplot as plt\n",
    "  plt.figure()\n",
    "  plt.step(range(len(class_features.T)), class_features.T, color='k', alpha=0.2, where='mid')\n",
    "  plt.plot(np.sqrt(np.exp(confusion)).T, 'r')\n",
    "  plt.show()\n",
    "  plt.gcf().savefig(dir_+\"signature.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Gifs\n",
    "id_ = np.random.choice(ids)\n",
    "print(id_)\n",
    "segment = segments[ids.index(id_)]\n",
    "import visuals\n",
    "visuals.single_segment_as_gif(segment)\n",
    "visuals.single_segment_reconstruction_as_gif(segment, vae, confidence=0.3)\n",
    "visuals.single_segment_rotations_reconstruction_as_gif(segment, vae, confidence=0.3)\n",
    "visuals.single_segment_degeneration_as_gif(segment, vae, confidence=0.3)\n",
    "visuals.single_segment_confidence_as_gif(segment, vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  dir_ = \"/tmp/online_matcher/visuals/reconstructions/\"\n",
    "  for class_name in classes_set:\n",
    "    print(class_name)\n",
    "    class_ids = [id_ for id_, class_ in zip(ids, classes) if class_ == class_name]\n",
    "    class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "    class_segments = np.array(segments)[class_indices]\n",
    "    class_features = np.array(features_nn)[class_indices]\n",
    "    class_confusion = np.array(confusion_nn)[class_indices]\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.step(range(len(class_features.T)), class_features.T, color='k', alpha=0.2, where='mid')\n",
    "    plt.plot(np.sqrt(np.exp(class_confusion)).T, 'r')\n",
    "    plt.show()\n",
    "    plt.gcf().savefig(dir_+class_name+\"_signature.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  # Include Rotated segments\n",
    "  for class_name in classes_set:\n",
    "    print(class_name)\n",
    "    class_ids = [id_ for id_, class_ in zip(ids, classes) if class_ == class_name]\n",
    "    class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "    class_segments = np.array(segments)[class_indices]\n",
    "  \n",
    "    from voxelize import create_rotations\n",
    "    class_rotated_segments = np.array(list(class_segments) + list(create_rotations(class_segments, N_ROTATION_ANGLES)))\n",
    "    from voxelize import voxelize\n",
    "    class_segments_vox, _ = voxelize(class_rotated_segments, VOXEL_SIDE)\n",
    "    class_features, confusion = vae.batch_encode([np.reshape(vox, MP.INPUT_SHAPE) for vox in class_segments_vox])\n",
    "    class_features = np.array(class_features)\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.step(range(len(class_features.T)), class_features.T, color='k', alpha=0.2, where='mid')\n",
    "    plt.plot(np.sqrt(np.exp(confusion)).T, 'r')\n",
    "    plt.show()\n",
    "    plt.gcf().savefig(dir_+class_name+\"_rotations_signature.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOTTING_SUPPORT:\n",
    "  from itertools import cycle\n",
    "  colors = cycle(['dodgerblue', 'gold', 'silver', 'tomato'])\n",
    "  plt.figure()\n",
    "  plt.title(\"Average absolute value of features, per class\")\n",
    "  plt.xlabel('feature #')\n",
    "  plt.ylabel('avg(abs(feature))')\n",
    "  for class_name, color_ in zip(classes_set, colors):\n",
    "    class_ids = [id_ for id_, class_ in zip(ids, classes) if class_ == class_name]\n",
    "    class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "    class_features = np.array(features_nn)[class_indices]\n",
    "    plt.plot(np.mean(np.abs(class_features), axis=0), marker='_', color=color_, label=class_name)\n",
    "    plt.hlines(np.mean(np.abs(class_features)),0,len(class_features[0])-1, linestyle='--', color=color_)\n",
    "  plt.show()\n",
    "  plt.legend()\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.title(\"Average confusion, per class\")\n",
    "  plt.xlabel('feature #')\n",
    "  plt.ylabel('sigma^2')\n",
    "  for class_name, color_ in zip(classes_set, colors):\n",
    "    class_ids = [id_ for id_, class_ in zip(ids, classes) if class_ == class_name]\n",
    "    class_indices = [ids.index(id_) for id_ in class_ids]\n",
    "    class_confusion = np.array(confusion_nn)[class_indices]\n",
    "    plt.plot(np.mean(np.exp(class_confusion), axis=0), marker='_', color=color_, label=class_name)\n",
    "    plt.hlines(np.mean(np.exp(class_confusion)),0,len(class_features[0])-1, linestyle='--', color=color_)\n",
    "plt.show()\n",
    "plt.legend()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_features(fnames_to_remove, fnames, features):\n",
    "    # Remove the autencoder features from the imported features if they already exist\n",
    "    for fname_to_remove in fnames_to_remove:\n",
    "        if fname_to_remove in fnames:\n",
    "            print(\"  Removing pre-existing feature \" + fname_to_remove)\n",
    "            for j, values in enumerate(features):\n",
    "                features[j] = np.delete(values, fnames.index(fname_to_remove))\n",
    "            fnames.remove(fname_to_remove)\n",
    "\n",
    "    assert len(fnames) == len(features[0])\n",
    "    \n",
    "def update_features(fnames_to_update, features_to_update, fnames, features):\n",
    "    assert len(fnames_to_update) == len(features_to_update[0])\n",
    "    # Remove the selected features if they already exist\n",
    "    remove_features(fnames_to_update, fnames, features)\n",
    "    # Add in the selected features\n",
    "    for fname in fnames_to_update: print(\"  Adding feature \" + fname)\n",
    "    for i, [f, ftu] in enumerate(zip(features, features_to_update)):\n",
    "        features[i] = np.concatenate([f, ftu])\n",
    "    fnames += fnames_to_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create copies of the original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if EXPORT_FEATURES:\n",
    "  updated_fnames = fnames[:]\n",
    "  updated_features = features[:]\n",
    "\n",
    "  print(fnames)\n",
    "  print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add/overwrite autoencoder features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if EXPORT_FEATURES:\n",
    "  # AE features\n",
    "  fnames_nn = ['autoencoder_feature'+str(i+1) for i in range(features_nn[0].shape[0])]\n",
    "  update_features(fnames_nn, features_nn, updated_fnames, updated_features)\n",
    "\n",
    "  # Scale features\n",
    "  sc_fnames = ['x_scale', 'y_scale', 'z_scale']\n",
    "  update_features(sc_fnames, features_voxel_scale, updated_fnames, updated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if EXPORT_FEATURES:\n",
    "  from load_segments import write_features\n",
    "  write_features(ids, updated_features, updated_fnames, filename=runs[run_index][features_file_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Features\n",
    "if CREATE_VISUALS:\n",
    "  from visuals import visuals_of_segments\n",
    "  visuals_of_segments(segments, ids, features=features_nn)\n",
    "  clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matches\n",
    "if CREATE_VISUALS:\n",
    "  from visuals import visuals_of_matches\n",
    "  visuals_of_matches(matches, segments, ids, features=features_nn)\n",
    "  clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Convert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONVERT_VARIABLE_NAMES = False\n",
    "name_to_var_dict = {}\n",
    "if CONVERT_VARIABLE_NAMES:\n",
    "  for var in vae.variables:\n",
    "    # Modify a few names\n",
    "    if 'LatentLayerWeights/' in var.name:\n",
    "      name = var.name.replace('LatentLayerWeights/', '')\n",
    "    elif 'ReconstructionLayerWeights/' in var.name:\n",
    "      name = var.name.replace('ReconstructionLayerWeights/', '')\n",
    "    # Leave other names unchanged\n",
    "    else:\n",
    "      name = var.name\n",
    "    name_to_var_dict[name] = var\n",
    "  temp_saver = tf.train.Saver(name_to_var_dict)\n",
    "  temp_saver.restore(vae.sess, SAVE_PATH)\n",
    "name_to_var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model and params\n",
    "if False:\n",
    "  vae.saver.save(vae.sess, SAVE_PATH)\n",
    "  with open(SAVE_DIR+MP_FILENAME, 'wb') as file:\n",
    "    pickle.dump(MP, file, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ml3]",
   "language": "python",
   "name": "Python [ml3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {
    "bb4397bb11c84525a852b17aaabffdc4": {
     "views": []
    },
    "bd44887d37014fcdb93112c4a9578106": {
     "views": []
    },
    "d279e0ce71ec4cbda4c2f2c8e24d1b96": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "e76e17549893445980c0385a1c4177d9": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
