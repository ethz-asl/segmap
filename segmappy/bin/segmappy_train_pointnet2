#!/usr/bin/env python
from __future__ import print_function
import numpy as np
import sys
import os

import ensure_segmappy_is_installed
from segmappy import Config
from segmappy import Dataset
from segmappy import Generator
from segmappy.tools.classifiertools import get_default_dataset, get_default_preprocessor

# read config file
configfile = "default_training.ini"

config = Config(configfile)

# add command line arguments to config
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--log")
parser.add_argument("--retrain")
parser.add_argument("--keep-best", action="store_true")
parser.add_argument("--matching-interval", type=int, default=10)
args = parser.parse_args()
config.log_name = args.log
config.retrain = args.retrain
config.keep_best = args.keep_best
config.matching_interval = args.matching_interval

# create or empty the model folder
if not os.path.exists(config.cnn_model_folder):
    os.makedirs(config.cnn_model_folder)
else:
    import glob

    model_files = glob.glob(os.path.join(config.cnn_model_folder, "*"))
    for model_file in model_files:
        os.remove(model_file)

# load preprocessor
preprocessor = get_default_preprocessor(config)

segments = []
segments_color = []
segments_class = []
classes = np.array([], dtype=np.int)
n_classes = 0
duplicate_classes = np.array([], dtype=np.int)
max_duplicate_class = 0
duplicate_ids = np.array([], dtype=np.int)

runs = config.cnn_train_folders.split(",")
for run in runs:
    dataset = get_default_dataset(config, run)

    run_segments, _, run_classes, run_n_classes, _, _, _ = dataset.load(
        preprocessor=preprocessor
    )
    run_duplicate_classes = dataset.duplicate_classes
    run_duplicate_ids = dataset.duplicate_ids

    run_classes += n_classes
    run_duplicate_classes += max_duplicate_class

    segments += run_segments
    segments_color += dataset.segments_color
    segments_class += dataset.segments_class
    classes = np.concatenate((classes, run_classes), axis=0)
    n_classes += run_n_classes
    duplicate_classes = np.concatenate(
        (duplicate_classes, run_duplicate_classes), axis=0
    )
    duplicate_ids = np.concatenate((duplicate_ids, run_duplicate_ids), axis=0)

    max_duplicate_class = np.max(duplicate_classes) + 1

# split so that the validation set contains entire sequences
train_fold = np.ones(classes.shape, dtype=np.int)
for c in np.unique(classes):
    dup_classes = duplicate_classes[classes == c]
    unique_dup_classes = np.unique(dup_classes)

    # choose for train the sequence with the largest last segment
    dup_sizes = []
    for dup_class in unique_dup_classes:
        dup_ids = np.where(dup_class == duplicate_classes)[0]
        last_id = np.max(dup_ids)
        dup_sizes.append(segments[last_id].shape[0])

    dup_keep = np.argmax(dup_sizes)

    # randomly distribute the others
    for i, dup_class in enumerate(unique_dup_classes):
        if i != dup_keep:
            if np.random.random() < config.validation_size:
                train_fold[duplicate_classes == dup_class] = 0

train_ids = np.where(train_fold == 1)[0]
valid_ids = np.where(train_fold == 0)[0]

# initialize preprocessor
preprocessor.init_segments(segments, segments_color, segments_class, classes)

# initialize segment batch generators
gen_train = Generator(
    preprocessor,
    train_ids,
    n_classes,
    train=True,
    batch_size=config.batch_size,
    shuffle=True,
    pointnet=True
)
gen_valid = Generator(
    preprocessor,
    valid_ids,
    n_classes,
    train=False,
    batch_size=config.batch_size,
    shuffle=True,
    pointnet=True
)

# get test dataset for testing matching performance
test_preprocessor = get_default_preprocessor(config)

test_dataset = Dataset(
    folder=config.cnn_test_folder,
    base_dir=config.base_dir,
    use_merges=config.use_merges,
    use_matches=config.use_matches,
    min_class_size=config.min_class_size,
    require_diff_points=config.require_diff_points,
    keep_match_thresh=config.keep_match_thresh,
    require_relevance=config.require_relevance,
    min_segment_size=0
)

test_segments, _, test_classes, test_n_classes, _, _, _ = \
    test_dataset.load(preprocessor=test_preprocessor)

test_duplicate_classes = test_dataset.duplicate_classes
test_duplicate_ids = test_dataset.duplicate_ids

test_preprocessor.init_segments(
    test_segments, test_dataset.segments_color, test_dataset.segments_class,
    test_classes)

gen_test = Generator(
    test_preprocessor,
    np.arange(len(test_segments)),
    test_n_classes,
    train=False,
    batch_size=config.batch_size,
    shuffle=False,
    pointnet=True
)

# precompute last ids and the corresponding segment sizes for sequences
test_last_ids = []
test_match_classes = []
test_last_sizes = {}
for cls in range(test_n_classes):
    test_sequences = test_duplicate_classes[test_classes == cls]
    test_unique_sequences = np.unique(test_sequences)

    if test_unique_sequences.size <= 1:
        continue

    for test_sequence in test_unique_sequences:
        test_segment_ids = np.where(test_sequence == test_duplicate_classes)[0]
        test_last_id = np.max(test_segment_ids)
        test_last_ids.append(test_last_id)
        test_last_sizes[test_sequence] = test_segments[test_last_id].shape[0]

    test_match_classes.append(cls)

test_last_ids = np.array(test_last_ids)
test_last_classes = test_classes[test_last_ids]
test_last_sequences = test_duplicate_classes[test_last_ids]

print("Training with %d segments" % gen_train.n_segments)
print("Validating with %d segments" % gen_valid.n_segments)
print("Testing with %d segments" % gen_test.n_segments)

import tensorflow as tf
import segmappy.models.model_pointnet2 as pointnet2
import segmappy.tools.tf_util as tf_util

tf.reset_default_graph()

NUM_POINT = 1024
BASE_LEARNING_RATE = 0.001
DECAY_STEP = 200000
DECAY_RATE = 0.7
BN_INIT_DECAY = 0.5
BN_DECAY_DECAY_RATE = 0.5
BN_DECAY_DECAY_STEP = float(DECAY_STEP)
BN_DECAY_CLIP = 0.99

def get_learning_rate(batch):
    learning_rate = tf.train.exponential_decay(
                        BASE_LEARNING_RATE,  # Base learning rate.
                        batch * config.batch_size,  # Current index into the dataset.
                        DECAY_STEP,          # Decay step.
                        DECAY_RATE,          # Decay rate.
                        staircase=True)
    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!
    #learning_rate = tf.constant(BASE_LEARNING_RATE)
    return learning_rate

def get_bn_decay(batch):
    bn_momentum = tf.train.exponential_decay(
                      BN_INIT_DECAY,
                      batch * config.batch_size,
                      BN_DECAY_DECAY_STEP,
                      BN_DECAY_DECAY_RATE,
                      staircase=True)
    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)
    #bn_decay = tf.constant(0.9)
    return bn_decay

pointclouds_pl, labels_pl = pointnet2.placeholder_inputs(NUM_POINT, n_classes)
is_training_pl = tf.placeholder(tf.bool, shape=())

batch = tf.get_variable('batch', [],
    initializer=tf.constant_initializer(0), trainable=False)
bn_decay = get_bn_decay(batch)
tf.summary.scalar('bn_decay', bn_decay, collections=["summary_batch"])

# Get model and loss
pred, end_points = pointnet2.get_model(
    pointclouds_pl, is_training_pl, n_classes, bn_decay=bn_decay)
pointnet2.get_loss(pred, labels_pl, end_points)
losses = tf.get_collection('losses')
total_loss = tf.add_n(losses, name='total_loss')
tf.summary.scalar('total_loss', total_loss, collections=["summary_batch"])
for l in losses + [total_loss]:
    tf.summary.scalar(l.op.name, l, collections=["summary_batch"])

correct = tf.equal(tf.argmax(pred, 1), tf.argmax(labels_pl, 1))
accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(config.batch_size)
tf.summary.scalar('accuracy', accuracy, collections=["summary_batch"])

learning_rate = get_learning_rate(batch)
tf.summary.scalar('learning_rate', learning_rate, collections=["summary_batch"])
optimizer = tf.train.AdamOptimizer(learning_rate)
train_op = optimizer.minimize(total_loss, global_step=batch)

descriptor = tf.get_default_graph().get_tensor_by_name(
    "OutputScope/descriptor_read:0")

means_tf = []
top95_tf = []
for i in range(10):
    means_tf.append(tf.placeholder(
    dtype=tf.float32, shape=(), name="means_" + str(i)))
    top95_tf.append(tf.placeholder(
    dtype=tf.float32, shape=(), name="top95_" + str(i)))

with tf.name_scope("summary_test"):
    for i in range(10):
        tf.summary.scalar(
        "means_" + str(i + 1), means_tf[i], collections=["summary_epoch"])
        tf.summary.scalar(
        "top95_" + str(i + 1), top95_tf[i], collections=["summary_epoch"])


saver = tf.train.Saver(max_to_keep=10)
summary_batch = tf.summary.merge_all("summary_batch")
summary_epoch = tf.summary.merge_all("summary_epoch")

tf_config = tf.ConfigProto()
tf_config.gpu_options.allow_growth = True
tf_config.allow_soft_placement = True
tf_config.log_device_placement = False

with tf.Session(config=tf_config) as sess:
    # tensorboard statistics
    if config.log_name:
        train_writer = tf.summary.FileWriter(
            os.path.join(config.log_path, config.log_name, "train"), sess.graph
        )
        valid_writer = tf.summary.FileWriter(
            os.path.join(config.log_path, config.log_name, "valid")
        )
        test_writer = tf.summary.FileWriter(
            os.path.join(config.log_path, config.log_name, "test")
        )

    # initialize all tf variables
    tf.global_variables_initializer().run()

    # remember best epoch accuracy
    if config.keep_best:
        best_accuracy = 0

    ops = {'pointclouds_pl': pointclouds_pl,
           'labels_pl': labels_pl,
           'is_training_pl': is_training_pl,
           'pred': pred,
           'loss': total_loss,
           'train_op': train_op,
           'summary_batch': summary_batch,
           'summary_epoch': summary_epoch,
           'step': batch,
           'end_points': end_points,
           'descriptor': descriptor}

    # sequence of train and validation batches
    batches = np.array([1] * gen_train.n_batches + [0] * gen_valid.n_batches)
    for epoch in range(0, config.n_epochs):
        train_loss = 0
        train_accuracy = 0
        train_step = 0

        valid_loss = 0
        valid_accuracy = 0
        valid_step = 0

        np.random.shuffle(batches)

        console_output_size = 0
        for step, train in enumerate(batches):
            if train:
                batch_segments, batch_classes = gen_train.next()

                feed_dict = {ops['pointclouds_pl']: batch_segments,
                             ops['labels_pl']: batch_classes,
                             ops['is_training_pl']: True}

                summary, step, _, batch_loss, batch_pred = sess.run([ops['summary_batch'],
                    ops['step'], ops['train_op'], ops['loss'], ops['pred']],
                    feed_dict=feed_dict)

                if config.log_name:
                    train_writer.add_summary(summary, step)

                correct = np.sum(
                    np.argmax(batch_pred, axis=1) == np.argmax(batch_classes, axis=1))

                train_loss += batch_loss
                train_accuracy += correct / float(batch_segments.shape[0])
                train_step += 1
            else:
                batch_segments, batch_classes = gen_valid.next()

                # calculate test loss and accuracy
                feed_dict = {ops['pointclouds_pl']: batch_segments,
                             ops['labels_pl']: batch_classes,
                             ops['is_training_pl']: False}

                summary, step, batch_loss, batch_pred = sess.run([ops['summary_batch'],
                    ops['step'], ops['loss'], ops['pred']], feed_dict=feed_dict)

                if config.log_name:
                    valid_writer.add_summary(summary, step)

                correct = np.sum(
                    np.argmax(batch_pred, axis=1) == np.argmax(batch_classes, axis=1))

                valid_loss += batch_loss
                valid_accuracy += correct / float(batch_segments.shape[0])
                valid_step += 1

            # print results
            sys.stdout.write("\b" * console_output_size)

            console_output = "epoch %2d " % epoch

            if train_step:
                console_output += "loss %.4f acc %.2f " % (
                    train_loss / train_step,
                    train_accuracy / train_step * 100
                )

            if valid_step:
                console_output += "v_loss %.4f v_acc %.2f" % (
                    valid_loss / valid_step,
                    valid_accuracy / valid_step * 100
                )

            console_output_size = len(console_output)

            sys.stdout.write(console_output)
            sys.stdout.flush()

        print()

        # calculate matching performance
        if (epoch + 1) % config.matching_interval == 0:
            test_features = []
            print("Calculating feature matching performance")
            for batch in range(gen_test.n_batches):
                batch_segments, _ = gen_test.next()

                # calculate test loss and accuracy
                feed_dict = {ops['pointclouds_pl']: batch_segments,
                             ops['is_training_pl']: False}

                batch_descriptors = sess.run(
                    [ops['descriptor']], feed_dict=feed_dict)[0]

                for batch_descriptor in batch_descriptors:
                    test_features.append(batch_descriptor)

            test_features = np.array(test_features)
            test_last_features = test_features[test_last_ids]

            ranks = []
            sizes = []
            for cls in test_match_classes:
                test_sequences = test_duplicate_classes[test_classes == cls]
                test_unique_sequences = np.unique(test_sequences)

                for test_sequence in test_unique_sequences:
                    test_segment_ids = np.where(
                        test_sequence == test_duplicate_classes)[0]

                    for test_segment_id in test_segment_ids:
                        dists = np.linalg.norm(
                            test_last_features - test_features[test_segment_id],
                            axis=1)
                        order_ids = np.argsort(dists)

                        found_self = False
                        for i, order_id in enumerate(order_ids):
                            if test_last_sequences[order_id] != test_sequence:
                                if test_last_classes[order_id] == cls:
                                    if found_self:
                                        ranks.append(i)
                                    else:
                                        ranks.append(i + 1)
                                    break
                            else:
                                found_self = True

                        sizes.append(
                            float(test_segments[test_segment_id].shape[0]) /
                                test_last_sizes[test_sequence])

            bin_edges = np.linspace(1.0/10, 1, 10)
            bins = []
            for i in range(bin_edges.size):
                bins.append([])

            for rank, size in zip(ranks, sizes):
                for i, bin_edge in enumerate(bin_edges):
                    if bin_edge >= size:
                        bins[i].append(rank)
                        break

            means = []
            top95 = []
            for bin in bins:
                bin = np.sort(bin)
                means.append(np.mean(bin))
                top95.append(bin[int(bin.size * 0.95)])

            console_log_means = 'Means:'
            console_log_top95 = 'Top95:'
            for i in range(bin_edges.size):
                console_log_means += ' %6.1f' % means[i]
                console_log_top95 += ' %6.1f' % top95[i]

            print(console_log_means)
            print(console_log_top95)

            if config.log_name:
                feed_dict = {}
                for i in range(10):
                    feed_dict[means_tf[i]] = means[i]
                    feed_dict[top95_tf[i]] = top95[i]

                summary = sess.run(summary_epoch, feed_dict=feed_dict)
                test_writer.add_summary(summary, step)

            # save epoch model
            if not config.keep_best or valid_accuracy > best_accuracy:
                if config.keep_best:
                    model_name = "model.ckpt"
                else:
                    model_name = "model-%d.ckpt" % step

                saver.save(sess, os.path.join(config.cnn_model_folder, model_name))
                tf.train.write_graph(
                    sess.graph.as_graph_def(), config.cnn_model_folder, "graph.pb"
                )

        # flush tensorboard log
        if config.log_name:
            train_writer.flush()
            valid_writer.flush()
            test_writer.flush()
